# No AI December Reflections

It started as a joke, but after finishing it, I started thinking about the impacts technology has on me and what actually matters.


Before December 2024, I connected with [James](https://x.com/james_s_bedford), who had exactly the same idea. We connected and made the website [noaidecember.com](https://noaidecember.com) to start the challenge.


During No AI December, I stopped using ChatGPT and Cursor editor in my day-to-day life.


I consider myself a heavy AI user when it comes to coding. After a week, I quickly realized that I relied on it maybe a little too much. You can notice it when your first thought is to, "Let's ask ChatGPT what it thinks". It's so convenient.

  
To get better answers, you need to structure the prompt the right way. Garbage in, garbage out. When I'm prompting, I'm not in fully problem-solving mode, but I'm describing what I want and predicting what information could be helpful for the AI.

  
As you can see, there is less thinking and a strong desire to know the answer quickly.


And that's cheating, right? Getting the answer quickly with the least amount of effort. It must be cheating, right?


I believe that there are two modes: I don't care, I just want the result, and I do care and I want to learn something here.

  
I wasn't explicit about this before, and I usually operated in the "I don't care" mode, which I consider harmful when it is the default mode. I want to grow over time and learn about new information, but importantly, I want to **retain** it.


My ability to **recall** information is strong signal that I learned something that I can build upon.  I don't want to be dumber than I was yesterday.


So I want to remind myself and remind you that human active thinking should be part of the process when we working with LLMs. It is very easy to mindlessly copy and paste the output and just quickly glance over it.


I would like to share a couple of observations (from No AI December) where I think effort plays a critical role.


My **ability to recall information from past chats** is non-existent. I feel like I'm just using cache memory when I'm dealing with ChatGPT. Everything is gone after a couple of minutes. This, on its own, is not a problem. Sometimes I want to fix something quickly, and there is a high probability that I won't need this information again. But I don't want to train myself to get quick answers. It's addictive.


My solution for now is note-taking when it comes to something I personally find useful. There is a higher probability that I will review it. I never reviewed my chats, I don't really want to because ChatGPT can spit out so much information in a single session.


Another change I noticed was in my ability to solve problems. Problems vary in complexity, and the harder it gets, the more patience and focus I need to make progress. I think my patience levels are much lower now because I expect to get answers more quickly. And there is the focus, which I think will be the "skill" of the future.


I don't have any solutions for this right now. I just don't rush it and allow myself to think about the problem more deeply. Again, there needs to be effort if I want to **understand** it and solve it.


I'm ending with a positive look on the future. We adapt quickly to new technology, and we appreciate it only at the beginning. You'll appreciate every piece of technology more if you stop using it for a while. I encourage you to try No AI December and share your thoughts.